{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f705f8-3b67-4a33-8ec0-89cbee3b46ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\miche\\anaconda3\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\miche\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\miche\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\miche\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\miche\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\miche\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\miche\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\miche\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.6.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 36.0 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, typing-inspection, pydantic-core, pydantic, ollama\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Uninstalling pydantic_core-2.20.1:\n",
      "      Successfully uninstalled pydantic_core-2.20.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "Successfully installed ollama-0.5.1 pydantic-2.11.7 pydantic-core-2.33.2 typing-extensions-4.14.1 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27af140a-83f9-444c-a48a-b6a0d2de6afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input_message: hello\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#记录模型开始思考的时间\u001b[39;00m\n\u001b[0;32m     11\u001b[0m time0\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 12\u001b[0m response\u001b[38;5;241m=\u001b[39mollama\u001b[38;5;241m.\u001b[39mchat(model,messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:Q1_str,},])\n\u001b[0;32m     13\u001b[0m A1_str\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#模型结束思考的时间\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:342\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    298\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    343\u001b[0m     ChatResponse,\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    346\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    347\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    348\u001b[0m       messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_copy_messages(messages)),\n\u001b[0;32m    349\u001b[0m       tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[0;32m    350\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    351\u001b[0m       think\u001b[38;5;241m=\u001b[39mthink,\n\u001b[0;32m    352\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    353\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    354\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    355\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    356\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    357\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    178\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:126\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[1;32m--> 126\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "#使用的模型\n",
    "model=\"deepseek-r1:8b\"\n",
    "\n",
    "#输入问题\n",
    "Q1_str=input(\"input_message:\")\n",
    "\n",
    "#记录模型开始思考的时间\n",
    "time0=time.time()\n",
    "response=ollama.chat(model,messages=[{\"role\":\"user\",\"content\":Q1_str,},])\n",
    "A1_str=response[\"message\"][\"content\"]\n",
    "\n",
    "#模型结束思考的时间\n",
    "time_AfterOutput=time.time()\n",
    "time_output=time_AfterOutput-time0\n",
    "v_output=len(A1_str)/time_output\n",
    "\n",
    "#打印输出\n",
    "print(A1_str)\n",
    "print()\n",
    "print(f\"Running model:{model}. Used time:{time_output}s. Input speed:{v_output}letter/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1807c18c-5a7b-4718-8f6d-c49deb57ee39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}' (93395996.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Your input:{len(input_message)} {\"/n\"} .   Used time:{time_input}.   Input speed{v_input}letter/s\")\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#记录输入时间\n",
    "time0=time.time()\n",
    "time_AfterInput=time.time()\n",
    "time_input=time_AfterInput-time0\n",
    "\n",
    "#计算输入速度\n",
    "v_input=len(input_message)/time_input\n",
    "\n",
    "#结果打印\n",
    "print(f\"Your input:{len(input_message)}.   Used time:{time_input}.   Input speed{v_input}letter/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecb787d-759f-4629-968b-70f2cfeee0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#实时输出\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      5\u001b[0m Q1_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow are you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeepseek-r1:14b\u001b[39m\u001b[38;5;124m'\u001b[39m,messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: Q1_str}],stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "#实时输出\n",
    "\n",
    "import ollama\n",
    "\n",
    "Q1_str=\"how are you \"\n",
    "\n",
    "response = ollama.chat(model='deepseek-r1:14b',messages=[{'role': 'user', 'content': Q1_str}],stream=True,)\n",
    "\n",
    "A1_str=chunk[\"message\"][\"content\"]\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59f845a1-66c4-49fe-a0db-f9231abcf3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image contains text from a poem, which is William Shakespeare's Sonnet XVIII. Here are the words I can see:\n",
      "\n",
      "\"Shakespeare, Sonnet XVIII\n",
      "William Shakespeare\n",
      "Sonnet XVIII\n",
      "\n",
      "Shall I compare thee to a summer's day?\n",
      "Thou art more lovely and more temperate:\n",
      "Rough winds do shake the darling buds,\n",
      "And summer's lease hath all too short a span.\n",
      "The eye of heaven shall ne'er look like that;\n",
      "Nor all the sons of earth shine half so bright.\n",
      "How much more gilded than a sun that's high,\n",
      "Or decorated with Encelad's swift course?\n",
      "Two of the fairest persons living in this fair land,\n",
      "In their first ages were but two hours old;\n",
      "Yet saw they not as many winters' fall;\n",
      "Their purer wings see not the need to fly.\n",
      "They sing of summer and of spring;\n",
      "Of sunny morns, or joyful spring noons;\n",
      "As have I seen surpassing far their beauty's mark.\n",
      "And in these times have come my eyes,\n",
      "To witness and to praise to what end ye do it.\n",
      "So hail, great powers, for as in heaven,\n",
      "A day like this cannot be so long.\" \n",
      "\n",
      "Running model:llava. Used time:15.340827941894531s. Input speed:64.53367469798627letter/s\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "#使用的模型\n",
    "model=\"llava\"\n",
    "\n",
    "#输入照片\n",
    "time0=time.time()\n",
    "with open(\"poem.jpeg\",\"rb\") as file:\n",
    "    response=ollama.chat(model=\"llava\",messages=[{\"role\":\"user\",\"content\":\"what is this.Tell me all word you can see.\",\"images\":[file.read()],},],) \n",
    "\n",
    "A1_image=response[\"message\"][\"content\"]\n",
    "\n",
    "time_AfterOutput=time.time()\n",
    "\n",
    "#计算时间\n",
    "time_output=time_AfterOutput-time0\n",
    "v_output=len(A1_image)/time_output\n",
    "\n",
    "print(A1_image)\n",
    "print()\n",
    "print(f\"Running model:{model}. Used time:{time_output}s. Input speed:{v_output}letter/s\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bcdf74b-a6fc-49df-b70b-7dbc7f32ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "你好！很高兴见到你，有什么我可以帮忙的吗？\n",
      "<\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "k\n",
      ">\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<\n",
      "/\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "k\n",
      ">\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "你\n",
      "好\n",
      "！\n",
      "很\n",
      "高\n",
      "兴\n",
      "见\n",
      "到\n",
      "你\n",
      "，\n",
      "有\n",
      "什\n",
      "么\n",
      "我\n",
      "可\n",
      "以\n",
      "帮\n",
      "忙\n",
      "的\n",
      "吗\n",
      "？\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "Q1_str=\"你好\"\n",
    "\n",
    "response=ollama.chat(model=\"deepseek-r1:14b\",messages=[{\"role\":\"user\",\"content\":Q1_str,},])\n",
    "\n",
    "A1_str=response[\"message\"][\"content\"]\n",
    "\n",
    "print(A1_str)\n",
    "\n",
    "for i in A1_str:\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc85e944-89b5-4525-9c9c-8c1b413cb092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeca9241-ff82-4cbe-a0c1-2813a6f1bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchObj.group() :  Cats are smarter than \n",
      "matchObj.group(1) :  Cats\n",
      "matchObj.group(2) :  smarter than\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "line = \"Cats are smarter than dogs\"\n",
    "# .* 表示任意匹配除换行符（\\n、\\r）之外的任何单个或多个字符\n",
    "# (.*?) 表示\"非贪婪\"模式，只保存第一个匹配到的子串\n",
    "matchObj = re.match( r'(.*) are (.*) ', line)\n",
    " \n",
    "if matchObj:\n",
    "   print (\"matchObj.group() : \", matchObj.group())\n",
    "   print (\"matchObj.group(1) : \", matchObj.group(1))\n",
    "   print (\"matchObj.group(2) : \", matchObj.group(2))\n",
    "else:\n",
    "   print (\"No match!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4941f551-f1bd-4e78-a4cf-77163762224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500cf3d6-c12a-4102-8e56-d92b80e3019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的翻译结果:\n",
      "你好你好 你怎么样了？\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_translation(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    提取输入文本中 </think> 标签后面的翻译结果。\n",
    "    如果找不到 </think> 标签，则返回原文本（或根据需要调整）。\n",
    "    \"\"\"\n",
    "    # 正则表达式：匹配 </think> 标签后紧跟的所有内容（包括换行符），并提取第一行非空文本\n",
    "    pattern = r'</think>\\s*(.+)'\n",
    "    match = re.search(pattern, input_text, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        # 若没有找到 </think> 标签，则可以直接返回原文本或做其他处理\n",
    "        return input_text.strip()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sample_input = \"\"\"输入：Hello hello how are you\n",
    "<think>\n",
    "好，用户让我把“Hello hello how are you”翻译成中文，并且只要翻译的结果。首先，我需要确认这句话的意思。“Hello hello”是重复问候，可能表达急切或强调，而“how are you”是常见的问候语。\n",
    "\n",
    "接下来，我得考虑如何准确地翻译。直接翻译的话，“你好你好 你怎么样了？”这样会比较直译，但中文里通常不会在口语中连续用两个“你好”，所以是否需要调整呢？不过用户要求只输出翻译结果，不需要加额外解释，所以我保持原样。\n",
    "\n",
    "再检查一下语法和语感。“你好你好”听起来有点重复，但在表达急切时是可以接受的。而“you”的翻译是“你”。整体来看，“你好你好 你怎么样了？”这句话在中文里是通顺的，符合用户的要求。\n",
    "</think>\n",
    "\n",
    "你好你好 你怎么样了？\"\"\"\n",
    "    \n",
    "    # 提取并输出翻译结果\n",
    "    translation = extract_translation(sample_input)\n",
    "    print(\"提取的翻译结果:\")\n",
    "    print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a0d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
